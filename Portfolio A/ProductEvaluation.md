Product Evaluation
===
### summary 
- how documented insights and feedback from evaluation
- how we used finding to refine final iterations of system

---

### Why Evaluate

We evaluate so we can judge how good each
release is and to systematically evaluate the product to make the necessary improvements required by the client. 
After the initial requirements, some subtle qualities of the system may have been missed either by us or the client, by evaluating 
we ensure the product is what the client envisioned. 
---

### Evaluation Approach 
Our evaluation approach consisted of; giving the client access to regularly deployed updates, 
this allowed them to use the product in their own real working environment and determine what needed changing with the product. 
Which they could then tell us about in a following meeting. While in these meeting we were about to conduct
our own evaluation, both by a questionnaire and observing how the client used the product.

We decided on a questionnaire, because our clients were the people who'd be using the product and new people would
be rarely using it, so it only made sense to just get their feedback on it. As part of our questionnaire, we wanted to 
find out how technically minded the current employees were, to ensure any new employees would find it just
as easy to get to grips with our product. 

When designing our questionnaire, we had to consider how much we could trust our findings, which meant we had to 
design a questionnaire that took into consideration...


After all this we would make the necessary changes and repeat the process.



- why we used this type of evaluation 

we wanted to find out... in order to pull out useful knowledge, making evolutionary system changes, iterative process
- Usefulness = utility x usability
    - Utility           
        - System should do what the users need
        - Need to get people to try it out
        - Assessed “in the wild”, in realistic context and environment
     - Usability
        -	Efficient, reliable, satisfying, learnable, maintainable, safe, secure, accessible, memorable, configurable, available

- Tasked-based evaluation
    -	Get real users to try out system functionality 
    -	Give them specific task to achieve using system
    -	Observe how well they get on 
    -	Document areas that need improvement

-	Quantitative and qualitative
        -	Tit – numerically
        -	Lit – measure literally

-	Bad questions
        -	Leading, tries to convince responder
        -	Loaded, put responder in difficult position
        -	Double-barrelled, has more than one element
        -	Absolute, using “always” or “ever”, we can always think of an exception
        -	Irrelevant, something we don’t care about, or can’t do anything about
        -	Truism, of course it is so pointless to ask
        -	Indeterminable, can’t gauge what’s being asked
        -	Indecipherable, hard for respondent to understand question
        -	Vague, doesn’t provide any useful info




-	What do we want to know we have to improve about UI
-	How usable is product
        -	Should meet client’s requirements
        -	Try it out, couple clicks
-	Efficient, reliable, satisfying, learnable, maintainable, safe, secure, accessible, memorable, configurable, available


#####Questionnaire
1.	On a scale of 1-10 how technically minded would you say you are? About how long did it take you to get to grips with MicroStrategy?
1.	Observe how well they get on with figuring out how the system works?
        1.	How many button clicks on average
        1.	How many trace backs
        1.	How many times asking for help/clarification
1.	Where would you have assumed the buttons to export the visualisation would have been?
1.	How well do you think the other BSDR employees (coders) will manage with getting use to this new visualisation software?
1.	On a scale of 1-10 how well does our product translate the features you needed from MicroStrategy?
1.	What features from MicroStrategy may be missing from our product?
1.	Do you have a need to be able to change the text on nodes or edges?


---


### 1st Feedback
first, filtering based on node weight

---

### 2nd Feedback
final one, weigh on nodes



---

### 