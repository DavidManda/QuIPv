Product Evaluation
===

### Why Evaluate
We evaluate so we can judge how good each
release is and to systematically evaluate the product to make the necessary improvements required by the client. 
After the initial requirements, some subtle qualities of the system may have been missed either by us or the client, by evaluating 
we ensure the product is what the client envisioned. 

### Evaluation Approach 
Our evaluation approach consisted of; giving the client access to regularly deployed updates, 
this allowed them to use the product in their own real working environment and determine what needed changing with the product. 
They could then email us about some fixes and tell us about the rest in a following meeting. 
While in these meeting we were about to conduct our own evaluation, both by a questionnaire and
observing how the client used the product.

We decided on a questionnaire because our client was already telling us most of what they thought the product was missing. 
Therefore by a questionnaire and observation we could find ways which we could improve the product that they hadn't originally imagined. For instance, 
we wanted to find out how technically minded the current users of our product were and how they were coping with understanding how to use it
, to ensure that future users wouldn't have problems getting to grips with our product. 

When designing our questionnaire, we had to consider how much we could trust our findings, which meant we had to 
design a questionnaire that didnt lead our client to answer that wasn't necessarily true, or was loaded and put them in a difficult position.
For example it would have been wrong to ask them if the product was easy to use, as this would put them in a difficult position.
Alternatively, we had to find this out ourselves, we accomplished this by task-based evaluation. We gave our clients a specific 
task and observed how they handled it.

After all the feedback we would continue developing the product, and make the changes requested as well as reflecting on the feedback
to determine if there was things not requested that needed changing, for instance, usability. We would then repeat this process to ensure 
the client for completely content with the product. 

#####Questionnaire
1.	On a scale of 1-10 how technically minded would you say you are? About how long did it take you to get to grips with MicroStrategy?
1.	Observe how well they get on with figuring out how the system works?
        1.	How many button clicks on average
        1.	How many trace backs
        1.	How many times asking for help/clarification
1.	Where would you have assumed the buttons to export the visualisation would have been?
1.	How well do you think the other BSDR employees (coders) will manage with getting use to this new visualisation software?
1.	On a scale of 1-10 how well does our product translate the features you needed from MicroStrategy?
1.	What features from MicroStrategy may be missing from our product?
1.	Do you have a need to be able to change the text on nodes or edges?



### 1st Feedback
-   all knew how to use programs basically and had the ability to work things out, but said they were not technically minded, 
but refereeing to knowing the back end of things.
-   Based of their use, the usability was great
-   Wanted
    -   silder, filtering based on node weight
    -   Data protection, safe access to database, user permission 
    -   outcome various numbers (edge cases)
    -   document/user guide
    -   export vis


### 2nd Feedback
-   doesn't work with large file sizes
-   seperate dataset, to drivers and outcomes
-   citation count/value on edges
-   delete rename project
-   select all/ deselect all for dataset
-   finish up user guide
-   nodes without outline, or smaller text to avoid overlapping
-   exporting visualisation 

-   happy that vis kept shape
-   already used product to prepare for presentations with their own clients


