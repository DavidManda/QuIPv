Product Evaluation
===

### Why Evaluate
We evaluate so we can judge how good each
release is and to systematically evaluate the product to make the necessary improvements required by the client. 
After the initial requirements, some subtle qualities of the system may have been missed either by us or the client, by evaluating 
we ensure the product is what the client envisioned. 

### Evaluation Approach 
Our evaluation approach consisted of; giving the client access to regularly deployed updates, 
this allowed them to use the product in their own real working environment and determine what needed changing with the product. 
They could then email us about some fixes and tell us about the rest in a following meeting. 
While in these meeting we were about to conduct our own evaluation, both by a questionnaire and
observing how the client used the product.

We decided on a questionnaire because our client was already telling us most of what they thought the product was missing. 
Therefore, by a questionnaire and with help from observations we could find ways in which to improve the product in ways that they hadn't originally imagined. For instance, 
we wanted to find out how technically minded the current users of our product were and how they were coping with understanding how to use it
, to ensure that future users wouldn't have problems getting to grips with our product. 

When designing our questionnaire, we had to consider how much we could trust our findings, which meant we had to 
design a questionnaire that didn't lead our client to answer that wasn't necessarily true or was loaded and put them in a difficult position.
For example, it would have been wrong to ask them if the product was easy to use, as this would put them in a difficult position.
Alternatively, we had to find this out ourselves, we accomplished this by task-based evaluation. We gave our clients a specific 
task and observed how they handled it.

After all the feedback we would continue developing the product, and make the changes requested as well as reflecting on the feedback
to determine if there were things not requested that needed changing, for instance, usability. We would then repeat this process to ensure 
the client for completely content with the product. 

###Questionnaire
1.	On a scale of 1-10 how technically minded would you say you are? About how long did it take you to get to grips with MicroStrategy?
1.	Observe how well they get on with figuring out how the system works?
        1.	How many button clicks on average
        1.	How many trace backs
        1.	How many times asking for help/clarification
1.	Where would you have assumed the buttons to export the visualisation would have been?
1.	How well do you think the other BSDR employees (coders) will manage with getting use to this new visualisation software?
1.	On a scale of 1-10 how well does our product translate the features you needed from MicroStrategy?
1.	What features from MicroStrategy may be missing from our product?
1.	Do you have a need to be able to change the text on nodes or edges?


### 1st Feedback
In our first round of evaluation, we were able to find out that our clients were not that technically minded, but could
use basic programs. When using our product, they were easily able to complete the tasks we asked. Therefore, we felt
the usability was great, any future users of our product should have no problems using it no matter their skill level. 

Our clients also fed back to us some features that were missing from our product. They needed a slider so that they could
filter the graph based on the weight of the nodes. This was then implemented into the system, but we were unable to fulfil
their other request of displaying the node weight due to some issue with the D3 library, however the client was able to work
around this. They also needed to make sure the data was protected; this issue was solved by enabling the client to make 
separate accounts on the product.

### 2nd Feedback
Our final round of evaluation was to ensure we were handing off a polished product to the client. After some more testing 
on their side, they had discovered the product could only handle files up to a certain size, meaning they were unable to 
upload some project. Even though they could screenshot the visualisation, which would render better quality than they
previously had, they wanted to have a button that exported the visualisation. These features along with a few other minor 
changes were added. However, there were still some things they'd have liked implemented that we were unable to do, 
but the product delivered addressed all the original requirements. 
In the end the client said they were very happy with the product, and showed us that they were already using it to present to
their own clients.









